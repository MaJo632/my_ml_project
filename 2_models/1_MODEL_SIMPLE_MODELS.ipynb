{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '3.11.3 (Python 3.11.3)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/Maria/.pyenv/pyenv-win/versions/3.11.3/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "import seaborn as sns\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, roc_auc_score, roc_curve, classification_report, f1_score, fbeta_score, make_scorer\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.compose import make_column_transformer\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.pipeline import Pipeline as imb_pipeline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "RSEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('data/cleaned/train.csv')\n",
    "# df_test = pd.read_csv('data/cleaned/test.csv')\n",
    "df = pd.read_csv('data/cleaned/train_extended.csv')\n",
    "df_test = pd.read_csv('data/cleaned/test_extended.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Target, Drop ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df['target']\n",
    "X_train = df.drop(['target', 'client_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split target from test data\n",
    "y_test = df_test['target']\n",
    "X_test = df_test.drop(['target', 'client_id'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical features\n",
    "num_features = [ 'consommation_sum', 'months_number']\n",
    "num_transformer = make_pipeline(\n",
    "        MinMaxScaler(), # no gaussian distribution? \n",
    "        # StandardScaler()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = make_column_transformer(\n",
    "        (num_transformer, num_features)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalanced Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Oversampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling the imbalanced\n",
    "ros = RandomOverSampler(random_state=RSEED)\n",
    "X_train, y_train = ros.fit_resample(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # SMOTE + ENN (Noise Reduction)\n",
    "smote_enn = SMOTEENN(random_state=42)\n",
    "\n",
    "X_train, y_train = smote_enn.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_logreg = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('logreg', LogisticRegression(random_state=RSEED))]\n",
    ")\n",
    "\n",
    "pipe_class = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('class', SGDClassifier(random_state=RSEED))]\n",
    ")\n",
    "\n",
    "pipe_knn = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('knn', KNeighborsClassifier())]\n",
    ")\n",
    "\n",
    "pipe_tree = Pipeline([\n",
    "    # ('preprocessor', preprocessor),\n",
    "    ('tree', DecisionTreeClassifier(random_state=RSEED))]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoring = make_scorer(fbeta_score, beta=2)\n",
    "scoring = 'roc_auc'\n",
    "cv = 5 #RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_logreg = {'logreg__penalty':('l1','l2'),\n",
    "                'logreg__C': [0.001, 0.01, 0.1, 1, 10],\n",
    "                'logreg__solver': ['liblinear', 'lbfgs', 'sag'],\n",
    "                'logreg__max_iter' : [1000, 500]\n",
    "               }\n",
    "\n",
    "grid_logreg = GridSearchCV(pipe_logreg, param_grid=param_logreg, cv=cv, scoring=scoring, \n",
    "                           verbose=5, n_jobs=-1) #evt. add error_score='raise'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show best parameters\n",
    "print('Best score:\\n{:.2f}'.format(grid_logreg.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(grid_logreg.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model (including fitted preprocessing steps) as best_model_logreg \n",
    "best_model_logreg = grid_logreg.best_estimator_\n",
    "best_model_logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted_logreg = best_model_logreg.predict(X_test)\n",
    "y_test_proba_logreg = best_model_logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_score(y_test, y_test_predicted_logreg)))\n",
    "print(\"Recall: {:.2f}\".format(recall_score(y_test, y_test_predicted_logreg)))\n",
    "print(\"Precision: {:.2f}\".format(precision_score(y_test, y_test_predicted_logreg)))\n",
    "print(f'Test ROC AUC Score: {roc_auc_score(y_test, y_test_proba_logreg)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdclass= SGDClassifier()\n",
    "sgdclass.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_class = {'class__penalty':('l1','l2'),\n",
    "                'class__alpha': [0.001, 0.01, 0.1, 1, 10],\n",
    "                'class__learning_rate': ['optimal', 'constant', 'invscaling'],\n",
    "                'class__loss': ['log_loss', 'modified_huber'],\n",
    "                'class__max_iter' : [1000, 500],\n",
    "                'class__eta0': [0.001, 0.01]\n",
    "               }\n",
    "\n",
    "grid_class = GridSearchCV(pipe_class, param_grid=param_class, cv=cv, scoring=scoring, \n",
    "                           verbose=5, n_jobs=-1) #evt. add error_score='raise'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_class.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show best parameters\n",
    "print('Best score:\\n{:.2f}'.format(grid_class.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(grid_class.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model (including fitted preprocessing steps) as best_model_class\n",
    "best_model_class = grid_class.best_estimator_\n",
    "best_model_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted_class = best_model_class.predict(X_test)\n",
    "y_test_proba_class = best_model_class.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_score(y_test, y_test_predicted_class)))\n",
    "print(\"Recall: {:.2f}\".format(recall_score(y_test, y_test_predicted_class)))\n",
    "print(\"Precision: {:.2f}\".format(precision_score(y_test, y_test_predicted_class)))\n",
    "print(f'Test ROC AUC Score: {roc_auc_score(y_test, y_test_proba_class)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### KNN\n",
    "# param_knn = {\"knn__n_neighbors\" : np.arange(2,50), #this actually defines the model you use\n",
    "#               \"knn__weights\" : [\"uniform\", \"distance\"],\n",
    "#               \"knn__p\" : [1, 2, 3],\n",
    "#               \"knn__algorithm\": [\"ball_tree\", \"kd_tree\", \"brute\"]\n",
    "#                }\n",
    "param_knn = {\"knn__n_neighbors\" : np.arange(2,50), #this actually defines the model you use\n",
    "              \"knn__p\" : [1, 2],\n",
    "               }\n",
    "             \n",
    "grid_knn = GridSearchCV(pipe_knn, param_grid=param_knn, cv=cv, scoring=scoring, \n",
    "                           verbose=5, n_jobs=-1) #evt. add error_score='raise'\n",
    "grid_knn.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show best parameters\n",
    "print('Best score:\\n{:.2f}'.format(grid_knn.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(grid_knn.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model (including fitted preprocessing steps) as best_model_knn\n",
    "best_model_knn = grid_knn.best_estimator_\n",
    "best_model_knn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted_knn = best_model_knn.predict(X_test)\n",
    "y_test_proba_knn = best_model_knn.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_score(y_test, y_test_predicted_knn)))\n",
    "print(\"Recall: {:.2f}\".format(recall_score(y_test, y_test_predicted_knn)))\n",
    "print(\"Precision: {:.2f}\".format(precision_score(y_test, y_test_predicted_knn)))\n",
    "print(f'Test ROC AUC Score: {roc_auc_score(y_test, y_test_proba_knn)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_tree = {\n",
    "    'tree__max_features': ['auto', 'sqrt', None] + list(np.arange(0.5, 1, 0.1)),\n",
    "    'tree__min_samples_split': [2, 5, 10],\n",
    "    'tree__max_depth': [None] + list(np.arange(3, 10).astype(int)),\n",
    "               }\n",
    "             \n",
    "grid_tree = GridSearchCV(pipe_tree, param_grid=param_tree, cv=cv, scoring=scoring, \n",
    "                           verbose=5, n_jobs=-1) #evt. add error_score='raise'\n",
    "grid_tree.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show best parameters\n",
    "print('Best score:\\n{:.2f}'.format(grid_tree.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(grid_tree.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model (including fitted preprocessing steps) as best_model_tree\n",
    "best_model_tree = grid_tree.best_estimator_\n",
    "best_model_tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted_tree = best_model_tree.predict(X_test)\n",
    "y_test_proba_tree = best_model_tree.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_score(y_test, y_test_predicted_tree)))\n",
    "print(\"Recall: {:.2f}\".format(recall_score(y_test, y_test_predicted_tree)))\n",
    "print(\"Precision: {:.2f}\".format(precision_score(y_test, y_test_predicted_tree)))\n",
    "print(f'Train ROC AUC Score: {roc_auc_score(y_test, y_test_proba_tree)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Without GridSearch ( = Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_tree = DecisionTreeClassifier(random_state=RSEED, max_depth=7)\n",
    "baseline_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted_bl = baseline_tree.predict(X_test)\n",
    "y_test_proba_bl = baseline_tree.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_score(y_test, y_test_predicted_bl)))\n",
    "print(\"Recall: {:.2f}\".format(recall_score(y_test, y_test_predicted_bl)))\n",
    "print(\"Precision: {:.2f}\".format(precision_score(y_test, y_test_predicted_bl)))\n",
    "print(f'Train ROC AUC Score: {roc_auc_score(y_test, y_test_proba_bl)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute FPR and TPR for the model to plot the roc_curve\n",
    "fpr_values_dt, tpr_values_dt, _ = roc_curve(y_test, y_test_proba_bl)  # Probability of class 1\n",
    "np.save(\"fpr_values_dt.npy\", fpr_values_dt)\n",
    "np.save(\"tpr_values_dt.npy\", tpr_values_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_metrics = np.array([accuracy_score(y_test, y_test_predicted_bl), precision_score(y_test, y_test_predicted_bl), recall_score(y_test, y_test_predicted_bl), f1_score(y_test, y_test_predicted_bl)]).round(2)\n",
    "dt_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,10))\n",
    "dectree_plot = plot_tree(baseline_tree, filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = ['logreg', 'classifier', 'knn', 'tree', 'baseline_tree']\n",
    "best_score_sum = [grid_logreg.best_score_, grid_class.best_score_, grid_knn.best_score_, grid_tree.best_score_, 'nan']\n",
    "test_accuracy_sum = [accuracy_score(y_test, y_test_predicted_logreg), accuracy_score(y_test, y_test_predicted_class), accuracy_score(y_test, y_test_predicted_knn), accuracy_score(y_test, y_test_predicted_tree), accuracy_score(y_test, y_test_predicted_bl)]\n",
    "test_recall_sum = [recall_score(y_test, y_test_predicted_logreg), recall_score(y_test, y_test_predicted_class), recall_score(y_test, y_test_predicted_knn),recall_score(y_test, y_test_predicted_tree), recall_score(y_test, y_test_predicted_bl)]\n",
    "test_precision_sum = [precision_score(y_test, y_test_predicted_logreg), precision_score(y_test, y_test_predicted_class), precision_score(y_test, y_test_predicted_knn), precision_score(y_test, y_test_predicted_tree), precision_score(y_test, y_test_predicted_bl)]\n",
    "test_roc_auc_sum = [roc_auc_score(y_test, y_test_proba_logreg), roc_auc_score(y_test, y_test_proba_class), roc_auc_score(y_test, y_test_proba_knn), roc_auc_score(y_test, y_test_proba_tree), roc_auc_score(y_test, y_test_proba_bl)]\n",
    "output = pd.DataFrame({\n",
    "    'model': index,\n",
    "    'best_score': best_score_sum,\n",
    "    'accuracy': test_accuracy_sum,\n",
    "    'recall': test_recall_sum,\n",
    "    'precision': test_precision_sum,\n",
    "    'roc_auc': test_roc_auc_sum\n",
    "})\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Undersampling**,\n",
    "**own scorer**\n",
    "\n",
    "|model|best_score|accuracy|recall|precision|roc_auc|\n",
    "|---|---|---|---|---|---|\n",
    "|    logreg|        0.87|    0.06|1.00|0.06|0.51|\n",
    "|    classifier|    0.87|    0.06|1.00|0.06|0.51|\n",
    "|    knn|            0.83|    0.47|0.73|0.07|0.62|\n",
    "|    tree|        0.88|    0.27|0.90|0.07|0.57|\n",
    "|    baseline_tree|nan|        0.61|0.77|0.10|0.76|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oversampling, own scorer**\n",
    "\n",
    "|model|best_score|accuracy|recall|precision|roc_auc|\n",
    "|---|---|---|---|---|---|\n",
    "|logreg|0.65|0.43|0.58|0.06|0.51|\n",
    "|classifier|0.83|0.06|1.00|0.06|0.50|\n",
    "|knn|0.98|0.90|0.07|0.08|0.52|\n",
    "|tree|0.99|0.90|0.07|0.07|0.50|\n",
    "|baseline_tree|nan|0.79|0.76|0.18|0.84|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
